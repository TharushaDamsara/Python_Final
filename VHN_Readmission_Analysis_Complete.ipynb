{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategic Patient Risk Stratification & Readmission Predictive Modeling\n",
    "## Vitality Health Network (VHN)\n",
    "\n",
    "**Course:** ITS 2122: Python for Data Science & AI (Semester 3 â€“ 2025)  \n",
    "**Dataset:** Diabetes 130-US Hospitals (1999â€“2008)  \n",
    "**Objective:** Analyze historical hospital data to identify drivers of 30-day readmissions and build a risk stratification system\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Phase 1: Data Ingestion & Clinical Sanitation](#phase1)\n",
    "2. [Phase 2: Data Enrichment via Web Scraping](#phase2)\n",
    "3. [Phase 3: Exploratory Data Analysis](#phase3)\n",
    "4. [Phase 4: Feature Engineering - Vitality Complexity Index](#phase4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from utils import (\n",
    "    calculate_vci_score,\n",
    "    categorize_vci_risk,\n",
    "    scrape_icd9_description,\n",
    "    audit_data_quality,\n",
    "    print_audit_summary,\n",
    "    plot_readmission_by_category,\n",
    "    plot_readmission_rate_by_category,\n",
    "    create_correlation_heatmap\n",
    ")\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase1'></a>\n",
    "# Phase 1: Data Ingestion & Clinical Sanitation\n",
    "\n",
    "In this phase, we perform professional healthcare data cleaning:\n",
    "- Load and audit the dataset\n",
    "- Handle missing values and data quality issues\n",
    "- Remove deceased patients\n",
    "- Convert data types appropriately\n",
    "- Remove duplicates\n",
    "- Document all cleaning decisions with clinical rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_files/diabetic_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load main dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_files/diabetic_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows Ã— \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load ID mappings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tharusha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tharusha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tharusha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tharusha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tharusha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data_files/diabetic_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load main dataset\n",
    "df = pd.read_csv('data_files/diabetic_data.csv')\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "\n",
    "# Load ID mappings\n",
    "id_mapping = pd.read_csv('data_files/IDs_mapping.csv')\n",
    "print(f\"\\nID mapping loaded: {id_mapping.shape[0]} mappings\")\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initial Data Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET STRUCTURE\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDATASET STRUCTURE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf\u001b[49m.info()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL SUMMARY - NUMERICAL FEATURES\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTATISTICAL SUMMARY - NUMERICAL FEATURES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf\u001b[49m.describe()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL SUMMARY - NUMERICAL FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CHECKING FOR '?' PLACEHOLDER VALUES\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      6\u001b[39m question_mark_counts = {}\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.columns:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df[col].dtype == \u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m      9\u001b[39m         count = (df[col] == \u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m).sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for '?' values (common placeholder in healthcare data)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECKING FOR '?' PLACEHOLDER VALUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "question_mark_counts = {}\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        count = (df[col] == '?').sum()\n",
    "        if count > 0:\n",
    "            question_mark_counts[col] = count\n",
    "\n",
    "if question_mark_counts:\n",
    "    qm_df = pd.DataFrame.from_dict(question_mark_counts, orient='index', columns=['Count'])\n",
    "    qm_df['Percentage'] = (qm_df['Count'] / len(df) * 100).round(2)\n",
    "    qm_df = qm_df.sort_values('Count', ascending=False)\n",
    "    print(qm_df)\n",
    "else:\n",
    "    print(\"No '?' values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Convert '?' to NaN\n",
    "\n",
    "**Clinical Rationale:** The '?' symbol is used as a placeholder for missing data in many healthcare datasets. Converting these to NaN allows proper handling with pandas missing data methods and prevents these values from being treated as valid categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "print(\"âœ“ Converted all '?' values to NaN\")\n",
    "\n",
    "# Verify conversion\n",
    "print(f\"\\nTotal NaN values in dataset: {df.isna().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Comprehensive Data Quality Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform audit using utility function\n",
    "audit_results = audit_data_quality(df)\n",
    "print_audit_summary(audit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Handle High Missingness Columns\n",
    "\n",
    "**Clinical Rationale:** Columns with >90% missing data provide minimal analytical value and can introduce bias. Common examples in healthcare data:\n",
    "- **weight**: Often not recorded consistently across facilities\n",
    "- **payer_code**: May not be captured in all systems\n",
    "- **medical_specialty**: Frequently missing in administrative data\n",
    "\n",
    "We document these as **data quality limitations** rather than attempting imputation, which would be clinically inappropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with >90% missingness\n",
    "high_missing_threshold = 0.90\n",
    "missing_pct = df.isna().sum() / len(df)\n",
    "high_missing_cols = missing_pct[missing_pct > high_missing_threshold].index.tolist()\n",
    "\n",
    "print(f\"Columns with >{high_missing_threshold*100}% missing data:\")\n",
    "for col in high_missing_cols:\n",
    "    pct = missing_pct[col] * 100\n",
    "    print(f\"  â€¢ {col}: {pct:.2f}% missing\")\n",
    "\n",
    "# Drop these columns\n",
    "if high_missing_cols:\n",
    "    df.drop(columns=high_missing_cols, inplace=True)\n",
    "    print(f\"\\nâœ“ Dropped {len(high_missing_cols)} columns with extreme missingness\")\n",
    "    print(f\"New dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"\\nNo columns exceed the 90% missingness threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Remove Deceased Patients\n",
    "\n",
    "**Clinical Rationale:** Patients who expired (died) during hospitalization cannot be readmitted and should be excluded from readmission analysis. Including them would:\n",
    "1. Artificially inflate the \"no readmission\" group\n",
    "2. Skew risk models toward end-of-life care patterns\n",
    "3. Violate the clinical definition of readmission risk\n",
    "\n",
    "We use the discharge_disposition_id mapping to identify deceased patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display discharge disposition mappings\n",
    "print(\"Discharge Disposition ID Mappings:\")\n",
    "print(id_mapping[id_mapping['Table'] == 'discharge_disposition_id'])\n",
    "\n",
    "# Identify deceased patient codes\n",
    "# Typically codes 11, 13, 14, 19, 20, 21 indicate expired/hospice/deceased\n",
    "deceased_codes = [11, 13, 14, 19, 20, 21]\n",
    "\n",
    "print(f\"\\nDeceased/Expired discharge codes: {deceased_codes}\")\n",
    "print(f\"Patients before removal: {len(df):,}\")\n",
    "\n",
    "# Count deceased patients\n",
    "deceased_count = df[df['discharge_disposition_id'].isin(deceased_codes)].shape[0]\n",
    "print(f\"Deceased patients identified: {deceased_count:,} ({deceased_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Remove deceased patients\n",
    "df = df[~df['discharge_disposition_id'].isin(deceased_codes)].copy()\n",
    "print(f\"Patients after removal: {len(df):,}\")\n",
    "print(f\"\\nâœ“ Removed {deceased_count:,} deceased patients from analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Convert to Appropriate Data Types\n",
    "\n",
    "**Clinical Rationale:** Proper data typing improves:\n",
    "- Memory efficiency\n",
    "- Analysis accuracy\n",
    "- Categorical analysis capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [\n",
    "    'race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id',\n",
    "    'admission_source_id', 'max_glu_serum', 'A1Cresult',\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n",
    "    'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "    'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone',\n",
    "    'change', 'diabetesMed', 'readmitted'\n",
    "]\n",
    "\n",
    "# Convert to category dtype\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "print(\"âœ“ Converted categorical columns to 'category' dtype\")\n",
    "print(f\"\\nMemory usage after conversion: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Remove Duplicate Records\n",
    "\n",
    "**Clinical Rationale:** Duplicate records can occur due to:\n",
    "- Data entry errors\n",
    "- System integration issues\n",
    "- Multiple submissions\n",
    "\n",
    "Duplicates must be removed to ensure accurate statistical analysis and prevent bias in predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicate_count:,}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"âœ“ Removed {duplicate_count:,} duplicate rows\")\n",
    "else:\n",
    "    print(\"âœ“ No duplicate rows found\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Phase 1 Summary\n",
    "\n",
    "**Data Cleaning Decisions Summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: DATA SANITATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nCleaning Actions Performed:\")\n",
    "print(\"  1. âœ“ Converted '?' placeholders to NaN\")\n",
    "print(f\"  2. âœ“ Dropped {len(high_missing_cols) if high_missing_cols else 0} columns with >90% missingness\")\n",
    "print(f\"  3. âœ“ Removed {deceased_count:,} deceased patients\")\n",
    "print(\"  4. âœ“ Converted categorical columns to appropriate dtype\")\n",
    "print(f\"  5. âœ“ Removed {duplicate_count:,} duplicate records\")\n",
    "print(f\"\\nFinal Clean Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase2'></a>\n",
    "# Phase 2: Data Enrichment via Web Scraping\n",
    "\n",
    "In this phase, we enhance diagnosis data with medical meaning by:\n",
    "- Identifying top 20 most frequent ICD-9 codes\n",
    "- Building ethical web scraping script\n",
    "- Scraping ICD-9 descriptions from public sources\n",
    "- Adding Primary_Diagnosis_Desc column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Analyze Primary Diagnosis Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diag_1 (primary diagnosis)\n",
    "print(\"=\" * 70)\n",
    "print(\"PRIMARY DIAGNOSIS (diag_1) ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get value counts\n",
    "diag1_counts = df['diag_1'].value_counts()\n",
    "print(f\"\\nTotal unique ICD-9 codes in diag_1: {len(diag1_counts):,}\")\n",
    "print(f\"Missing values: {df['diag_1'].isna().sum():,}\")\n",
    "\n",
    "# Top 20 most frequent codes\n",
    "top_20_codes = diag1_counts.head(20)\n",
    "print(\"\\nTop 20 Most Frequent ICD-9 Codes:\")\n",
    "print(top_20_codes)\n",
    "\n",
    "# Calculate coverage\n",
    "coverage = (top_20_codes.sum() / len(df)) * 100\n",
    "print(f\"\\nTop 20 codes cover {coverage:.2f}% of all patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 codes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_20_codes.plot(kind='bar', color='steelblue', alpha=0.8)\n",
    "plt.title('Top 20 Most Frequent Primary Diagnosis Codes (ICD-9)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('ICD-9 Code', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Web Scraping ICD-9 Descriptions\n",
    "\n",
    "**Ethical Scraping Practices:**\n",
    "- Using public medical coding reference (ICD9Data.com)\n",
    "- Implementing delays between requests (time.sleep)\n",
    "- Setting appropriate User-Agent headers\n",
    "- Handling errors gracefully\n",
    "- Respecting rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Get list of top 20 codes\n",
    "top_20_list = top_20_codes.index.tolist()\n",
    "\n",
    "print(\"Starting web scraping for top 20 ICD-9 codes...\")\n",
    "print(\"This may take a few moments due to ethical delay between requests.\\n\")\n",
    "\n",
    "# Dictionary to store code descriptions\n",
    "icd9_descriptions = {}\n",
    "\n",
    "# Scrape descriptions\n",
    "for i, code in enumerate(top_20_list, 1):\n",
    "    print(f\"[{i}/20] Scraping ICD-9 code: {code}...\", end=' ')\n",
    "    \n",
    "    description = scrape_icd9_description(code, delay=0.5)\n",
    "    icd9_descriptions[str(code)] = description\n",
    "    \n",
    "    print(f\"âœ“ {description}\")\n",
    "\n",
    "print(\"\\nâœ“ Web scraping complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scraped descriptions\n",
    "print(\"=\" * 70)\n",
    "print(\"SCRAPED ICD-9 DESCRIPTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "desc_df = pd.DataFrame.from_dict(icd9_descriptions, orient='index', columns=['Description'])\n",
    "desc_df.index.name = 'ICD-9 Code'\n",
    "desc_df['Frequency'] = desc_df.index.map(lambda x: diag1_counts.get(x, 0))\n",
    "print(desc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Add Primary_Diagnosis_Desc Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping function\n",
    "def map_diagnosis_description(code):\n",
    "    \"\"\"Map ICD-9 code to description, label non-top-20 as 'Other'\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return 'Missing'\n",
    "    code_str = str(code)\n",
    "    return icd9_descriptions.get(code_str, 'Other')\n",
    "\n",
    "# Apply mapping\n",
    "df['Primary_Diagnosis_Desc'] = df['diag_1'].apply(map_diagnosis_description)\n",
    "\n",
    "print(\"âœ“ Added 'Primary_Diagnosis_Desc' column\")\n",
    "print(f\"\\nValue distribution:\")\n",
    "print(df['Primary_Diagnosis_Desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Phase 2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: DATA ENRICHMENT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nEnrichment Actions Performed:\")\n",
    "print(f\"  1. âœ“ Identified top 20 ICD-9 codes (covering {coverage:.2f}% of patients)\")\n",
    "print(f\"  2. âœ“ Scraped {len(icd9_descriptions)} ICD-9 descriptions from ICD9Data.com\")\n",
    "print(\"  3. âœ“ Implemented ethical scraping practices (delays, headers, error handling)\")\n",
    "print(\"  4. âœ“ Added 'Primary_Diagnosis_Desc' column\")\n",
    "print(\"  5. âœ“ Labeled non-top-20 diagnoses as 'Other'\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase3'></a>\n",
    "# Phase 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this phase, we discover patterns and insights driving readmissions:\n",
    "- Analyze class imbalance\n",
    "- Visualize demographic patterns\n",
    "- Compare medication usage\n",
    "- Analyze operational metrics\n",
    "- Create correlation analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze readmission distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"READMISSION CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "readmit_counts = df['readmitted'].value_counts()\n",
    "readmit_pct = df['readmitted'].value_counts(normalize=True) * 100\n",
    "\n",
    "readmit_summary = pd.DataFrame({\n",
    "    'Count': readmit_counts,\n",
    "    'Percentage': readmit_pct.round(2)\n",
    "})\n",
    "\n",
    "print(readmit_summary)\n",
    "\n",
    "# Calculate 30-day readmission rate\n",
    "readmit_30_rate = (df['readmitted'] == '<30').sum() / len(df) * 100\n",
    "print(f\"\\n30-Day Readmission Rate: {readmit_30_rate:.2f}%\")\n",
    "print(f\"This is {'ABOVE' if readmit_30_rate > 15 else 'BELOW'} the typical 15-20% benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "readmit_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c', '#3498db'], alpha=0.8)\n",
    "axes[0].set_title('Readmission Status Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(readmit_counts, labels=readmit_counts.index, autopct='%1.1f%%',\n",
    "            colors=['#2ecc71', '#e74c3c', '#3498db'], startangle=90)\n",
    "axes[1].set_title('Readmission Status Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation: The dataset shows class imbalance, with <30 day readmissions\")\n",
    "print(\"being the minority class. This is clinically realistic and will inform our\")\n",
    "print(\"risk stratification approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Demographic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Age Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readmission by age group\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='age', hue='readmitted', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Readmission Status by Age Group', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Age Group', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Readmitted', loc='upper left')\n",
    "\n",
    "# Readmission rate by age\n",
    "age_readmit_rate = df.groupby('age')['readmitted'].apply(\n",
    "    lambda x: (x == '<30').sum() / len(x) * 100\n",
    ").sort_index()\n",
    "\n",
    "age_readmit_rate.plot(kind='bar', ax=axes[1], color='coral', alpha=0.8)\n",
    "axes[1].set_title('30-Day Readmission Rate by Age Group', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Age Group', fontsize=12)\n",
    "axes[1].set_ylabel('Readmission Rate (%)', fontsize=12)\n",
    "axes[1].axhline(y=readmit_30_rate, color='red', linestyle='--', label=f'Overall: {readmit_30_rate:.2f}%')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Interpretation:\")\n",
    "print(f\"Highest readmission rate: {age_readmit_rate.idxmax()} ({age_readmit_rate.max():.2f}%)\")\n",
    "print(f\"Lowest readmission rate: {age_readmit_rate.idxmin()} ({age_readmit_rate.min():.2f}%)\")\n",
    "print(\"Age appears to be a significant factor in readmission risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readmission by gender\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='gender', hue='readmitted', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Readmission Status by Gender', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Gender', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].legend(title='Readmitted')\n",
    "\n",
    "# Readmission rate by gender\n",
    "gender_readmit_rate = df.groupby('gender')['readmitted'].apply(\n",
    "    lambda x: (x == '<30').sum() / len(x) * 100\n",
    ")\n",
    "\n",
    "gender_readmit_rate.plot(kind='bar', ax=axes[1], color='skyblue', alpha=0.8)\n",
    "axes[1].set_title('30-Day Readmission Rate by Gender', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Gender', fontsize=12)\n",
    "axes[1].set_ylabel('Readmission Rate (%)', fontsize=12)\n",
    "axes[1].axhline(y=readmit_30_rate, color='red', linestyle='--', label=f'Overall: {readmit_30_rate:.2f}%')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Interpretation:\")\n",
    "print(gender_readmit_rate)\n",
    "print(f\"Gender difference in readmission: {abs(gender_readmit_rate.iloc[0] - gender_readmit_rate.iloc[1]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Race Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readmission by race\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count plot\n",
    "race_order = df['race'].value_counts().index\n",
    "sns.countplot(data=df, x='race', hue='readmitted', ax=axes[0], palette='Set2', order=race_order)\n",
    "axes[0].set_title('Readmission Status by Race', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Race', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Readmitted', loc='upper right')\n",
    "\n",
    "# Readmission rate by race\n",
    "race_readmit_rate = df.groupby('race')['readmitted'].apply(\n",
    "    lambda x: (x == '<30').sum() / len(x) * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "race_readmit_rate.plot(kind='bar', ax=axes[1], color='lightcoral', alpha=0.8)\n",
    "axes[1].set_title('30-Day Readmission Rate by Race', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Race', fontsize=12)\n",
    "axes[1].set_ylabel('Readmission Rate (%)', fontsize=12)\n",
    "axes[1].axhline(y=readmit_30_rate, color='red', linestyle='--', label=f'Overall: {readmit_30_rate:.2f}%')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Interpretation:\")\n",
    "print(race_readmit_rate)\n",
    "print(\"\\nRacial disparities in readmission rates may reflect social determinants\")\n",
    "print(\"of health, access to care, and systemic healthcare inequities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Medication Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create medication categories\n",
    "def categorize_medication(row):\n",
    "    \"\"\"Categorize patients by medication usage\"\"\"\n",
    "    insulin = row.get('insulin', 'No')\n",
    "    diabetesMed = row.get('diabetesMed', 'No')\n",
    "    \n",
    "    if insulin in ['Down', 'Steady', 'Up']:\n",
    "        return 'Insulin User'\n",
    "    elif diabetesMed == 'Yes':\n",
    "        return 'Oral Medication'\n",
    "    else:\n",
    "        return 'No Medication'\n",
    "\n",
    "df['Medication_Category'] = df.apply(categorize_medication, axis=1)\n",
    "\n",
    "print(\"Medication Category Distribution:\")\n",
    "print(df['Medication_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze readmission by medication category\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count plot\n",
    "med_order = df['Medication_Category'].value_counts().index\n",
    "sns.countplot(data=df, x='Medication_Category', hue='readmitted', ax=axes[0], \n",
    "              palette='Set2', order=med_order)\n",
    "axes[0].set_title('Readmission Status by Medication Category', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Medication Category', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "axes[0].legend(title='Readmitted')\n",
    "\n",
    "# Readmission rate\n",
    "med_readmit_rate = df.groupby('Medication_Category')['readmitted'].apply(\n",
    "    lambda x: (x == '<30').sum() / len(x) * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "med_readmit_rate.plot(kind='bar', ax=axes[1], color='mediumseagreen', alpha=0.8)\n",
    "axes[1].set_title('30-Day Readmission Rate by Medication Category', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Medication Category', fontsize=12)\n",
    "axes[1].set_ylabel('Readmission Rate (%)', fontsize=12)\n",
    "axes[1].axhline(y=readmit_30_rate, color='red', linestyle='--', label=f'Overall: {readmit_30_rate:.2f}%')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Interpretation:\")\n",
    "print(med_readmit_rate)\n",
    "print(\"\\nMedication management appears to influence readmission risk.\")\n",
    "print(\"Insulin users may represent more severe diabetes cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Operational Metrics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Time in Hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time in hospital by readmission status\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df, x='readmitted', y='time_in_hospital', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Time in Hospital by Readmission Status', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[0].set_ylabel('Days in Hospital', fontsize=12)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df, x='readmitted', y='time_in_hospital', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Distribution of Hospital Stay Length', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[1].set_ylabel('Days in Hospital', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nðŸ“Š Time in Hospital Statistics by Readmission Status:\")\n",
    "print(df.groupby('readmitted')['time_in_hospital'].describe())\n",
    "\n",
    "print(\"\\nClinical Interpretation: Longer hospital stays may indicate higher\")\n",
    "print(\"complexity and could be associated with readmission risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Number of Lab Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lab procedures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df, x='readmitted', y='num_lab_procedures', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Number of Lab Procedures by Readmission Status', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Lab Procedures', fontsize=12)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df, x='readmitted', y='num_lab_procedures', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Distribution of Lab Procedures', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Lab Procedures', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nðŸ“Š Lab Procedures Statistics by Readmission Status:\")\n",
    "print(df.groupby('readmitted')['num_lab_procedures'].describe())\n",
    "\n",
    "print(\"\\nClinical Interpretation: More lab procedures may indicate higher acuity\")\n",
    "print(\"and more complex medical conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Number of Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze medications\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df, x='readmitted', y='num_medications', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Number of Medications by Readmission Status', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Medications', fontsize=12)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df, x='readmitted', y='num_medications', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Distribution of Medications', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Readmission Status', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Medications', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nðŸ“Š Medications Statistics by Readmission Status:\")\n",
    "print(df.groupby('readmitted')['num_medications'].describe())\n",
    "\n",
    "print(\"\\nClinical Interpretation: Higher medication counts suggest polypharmacy,\")\n",
    "print(\"which can increase complexity of care and potential for adverse events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key numerical features for correlation analysis\n",
    "numerical_features = [\n",
    "    'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "    'num_medications', 'number_outpatient', 'number_emergency',\n",
    "    'number_inpatient', 'number_diagnoses'\n",
    "]\n",
    "\n",
    "# Create correlation heatmap\n",
    "corr_matrix = create_correlation_heatmap(df, columns=numerical_features, figsize=(12, 10))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Correlations:\")\n",
    "print(\"\\nStrongest positive correlations:\")\n",
    "# Get upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "correlations = upper_tri.stack().sort_values(ascending=False)\n",
    "print(correlations.head(5))\n",
    "\n",
    "print(\"\\nClinical Interpretation: Understanding feature correlations helps identify\")\n",
    "print(\"multicollinearity and reveals relationships between operational metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Phase 3 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: EXPLORATORY DATA ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  1. âœ“ 30-Day Readmission Rate: {readmit_30_rate:.2f}%\")\n",
    "print(f\"  2. âœ“ Class imbalance present (realistic for healthcare data)\")\n",
    "print(\"  3. âœ“ Age shows variation in readmission risk\")\n",
    "print(\"  4. âœ“ Medication category influences readmission patterns\")\n",
    "print(\"  5. âœ“ Operational metrics (time, labs, meds) show associations\")\n",
    "print(\"  6. âœ“ Correlations identified between clinical features\")\n",
    "print(\"\\nThese insights will inform the VCI risk stratification model.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phase4'></a>\n",
    "# Phase 4: Feature Engineering - Vitality Complexity Index (VCI)\n",
    "\n",
    "In this phase, we build a patient risk scoring algorithm inspired by the LACE Index:\n",
    "- **L**: Length of Stay (0-7 points)\n",
    "- **A**: Acuity of Admission (0-3 points)\n",
    "- **C**: Comorbidity Burden (0-5 points)\n",
    "- **E**: Emergency Visits (0-5 points)\n",
    "\n",
    "**Total VCI Score Range**: 0-20 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Calculate VCI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VCI score for each patient\n",
    "print(\"Calculating Vitality Complexity Index (VCI) scores...\")\n",
    "\n",
    "df['VCI_Score'] = df.apply(calculate_vci_score, axis=1)\n",
    "\n",
    "print(\"âœ“ VCI scores calculated\")\n",
    "print(f\"\\nVCI Score Statistics:\")\n",
    "print(df['VCI_Score'].describe())\n",
    "\n",
    "# Display distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['VCI_Score'], bins=21, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of VCI Scores', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('VCI Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(df['VCI_Score'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {df[\"VCI_Score\"].mean():.2f}')\n",
    "plt.axvline(df['VCI_Score'].median(), color='green', linestyle='--',\n",
    "            label=f'Median: {df[\"VCI_Score\"].median():.2f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Categorize into Risk Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize patients into risk groups\n",
    "df['VCI_Risk_Category'] = df['VCI_Score'].apply(categorize_vci_risk)\n",
    "\n",
    "print(\"Risk Category Distribution:\")\n",
    "risk_dist = df['VCI_Risk_Category'].value_counts()\n",
    "print(risk_dist)\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "print((risk_dist / len(df) * 100).round(2))\n",
    "\n",
    "# Visualize risk categories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "risk_order = ['Low', 'Medium', 'High']\n",
    "risk_colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "risk_dist_ordered = df['VCI_Risk_Category'].value_counts().reindex(risk_order)\n",
    "risk_dist_ordered.plot(kind='bar', ax=axes[0], color=risk_colors, alpha=0.8)\n",
    "axes[0].set_title('VCI Risk Category Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Risk Category', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(risk_dist_ordered, labels=risk_order, autopct='%1.1f%%',\n",
    "            colors=risk_colors, startangle=90)\n",
    "axes[1].set_title('VCI Risk Category Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Validate VCI Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate readmission rates by risk category\n",
    "print(\"=\" * 70)\n",
    "print(\"VCI VALIDATION: 30-DAY READMISSION RATES BY RISK CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "risk_readmit_analysis = df.groupby('VCI_Risk_Category')['readmitted'].apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Total_Patients': len(x),\n",
    "        'Readmitted_30d': (x == '<30').sum(),\n",
    "        'Readmission_Rate_%': (x == '<30').sum() / len(x) * 100\n",
    "    })\n",
    ").reindex(risk_order)\n",
    "\n",
    "print(risk_readmit_analysis)\n",
    "\n",
    "# Calculate risk ratio\n",
    "low_rate = risk_readmit_analysis.loc['Low', 'Readmission_Rate_%']\n",
    "high_rate = risk_readmit_analysis.loc['High', 'Readmission_Rate_%']\n",
    "risk_ratio = high_rate / low_rate if low_rate > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“Š Key Validation Metrics:\")\n",
    "print(f\"  â€¢ Low Risk Readmission Rate: {low_rate:.2f}%\")\n",
    "print(f\"  â€¢ High Risk Readmission Rate: {high_rate:.2f}%\")\n",
    "print(f\"  â€¢ Risk Ratio (High/Low): {risk_ratio:.2f}x\")\n",
    "\n",
    "if risk_ratio > 1.5:\n",
    "    print(\"\\nâœ… VCI SUCCESSFULLY STRATIFIES RISK\")\n",
    "    print(\"High-risk patients have significantly higher readmission rates.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ VCI shows moderate risk stratification\")\n",
    "    print(\"Further refinement may be needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize readmission rates by risk category\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Stacked bar chart\n",
    "readmit_by_risk = pd.crosstab(df['VCI_Risk_Category'], df['readmitted'], normalize='index') * 100\n",
    "readmit_by_risk = readmit_by_risk.reindex(risk_order)\n",
    "readmit_by_risk.plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                     color=['#2ecc71', '#e74c3c', '#3498db'], alpha=0.8)\n",
    "axes[0].set_title('Readmission Status Distribution by Risk Category', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('VCI Risk Category', fontsize=12)\n",
    "axes[0].set_ylabel('Percentage (%)', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "axes[0].legend(title='Readmitted', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 30-day readmission rate comparison\n",
    "readmit_30_by_risk = risk_readmit_analysis['Readmission_Rate_%']\n",
    "bars = axes[1].bar(risk_order, readmit_30_by_risk, color=risk_colors, alpha=0.8)\n",
    "axes[1].set_title('30-Day Readmission Rate by VCI Risk Category', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('VCI Risk Category', fontsize=12)\n",
    "axes[1].set_ylabel('Readmission Rate (%)', fontsize=12)\n",
    "axes[1].axhline(y=readmit_30_rate, color='red', linestyle='--', \n",
    "                label=f'Overall: {readmit_30_rate:.2f}%')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 VCI Score vs Readmission Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze readmission rate across VCI score spectrum\n",
    "vci_readmit_by_score = df.groupby('VCI_Score')['readmitted'].apply(\n",
    "    lambda x: (x == '<30').sum() / len(x) * 100\n",
    ").sort_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(vci_readmit_by_score.index, vci_readmit_by_score.values, \n",
    "         marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.title('30-Day Readmission Rate by VCI Score', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('VCI Score', fontsize=12)\n",
    "plt.ylabel('Readmission Rate (%)', fontsize=12)\n",
    "plt.axhline(y=readmit_30_rate, color='red', linestyle='--', alpha=0.7,\n",
    "            label=f'Overall Average: {readmit_30_rate:.2f}%')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Clinical Interpretation:\")\n",
    "print(\"The trend line shows how readmission risk changes across the VCI spectrum.\")\n",
    "print(\"An upward trend validates the VCI as an effective risk stratification tool.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze contribution of each VCI component\n",
    "print(\"=\" * 70)\n",
    "print(\"VCI COMPONENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate individual component scores\n",
    "def get_length_score(days):\n",
    "    if days < 1: return 0\n",
    "    elif 1 <= days <= 4: return 1\n",
    "    elif 5 <= days <= 13: return 4\n",
    "    else: return 7\n",
    "\n",
    "df['L_Score'] = df['time_in_hospital'].apply(get_length_score)\n",
    "df['A_Score'] = df['admission_type_id'].apply(lambda x: 3 if x in [1, 2] else 0)\n",
    "\n",
    "# Analyze component distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Length of Stay\n",
    "df['L_Score'].value_counts().sort_index().plot(kind='bar', ax=axes[0,0], \n",
    "                                                color='skyblue', alpha=0.8)\n",
    "axes[0,0].set_title('L: Length of Stay Score Distribution', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Score')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Acuity\n",
    "df['A_Score'].value_counts().sort_index().plot(kind='bar', ax=axes[0,1],\n",
    "                                                color='lightcoral', alpha=0.8)\n",
    "axes[0,1].set_title('A: Acuity of Admission Score Distribution', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Score')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "\n",
    "# Emergency Visits\n",
    "df['number_emergency'].value_counts().sort_index().head(10).plot(kind='bar', ax=axes[1,0],\n",
    "                                                                   color='lightgreen', alpha=0.8)\n",
    "axes[1,0].set_title('E: Emergency Visits Distribution', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Number of Emergency Visits')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "\n",
    "# Number of Diagnoses (proxy for Comorbidity)\n",
    "df['number_diagnoses'].value_counts().sort_index().plot(kind='bar', ax=axes[1,1],\n",
    "                                                         color='plum', alpha=0.8)\n",
    "axes[1,1].set_title('C: Number of Diagnoses Distribution', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Number of Diagnoses')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach component contributes to the overall VCI score, capturing different\")\n",
    "print(\"aspects of patient complexity and readmission risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Phase 4 Summary & Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4: VITALITY COMPLEXITY INDEX (VCI) - COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nVCI Implementation Summary:\")\n",
    "print(f\"  âœ“ Calculated VCI scores for {len(df):,} patients\")\n",
    "print(f\"  âœ“ Score range: {df['VCI_Score'].min()}-{df['VCI_Score'].max()} (possible: 0-20)\")\n",
    "print(f\"  âœ“ Mean VCI: {df['VCI_Score'].mean():.2f}\")\n",
    "print(f\"  âœ“ Median VCI: {df['VCI_Score'].median():.2f}\")\n",
    "print(\"\\nRisk Stratification Results:\")\n",
    "for risk in risk_order:\n",
    "    count = risk_dist_ordered[risk]\n",
    "    pct = (count / len(df)) * 100\n",
    "    rate = risk_readmit_analysis.loc[risk, 'Readmission_Rate_%']\n",
    "    print(f\"  â€¢ {risk} Risk: {count:,} patients ({pct:.1f}%) - {rate:.2f}% readmission rate\")\n",
    "\n",
    "print(f\"\\nâœ… VCI VALIDATION: Risk Ratio = {risk_ratio:.2f}x\")\n",
    "print(\"   High-risk patients are {:.1f}x more likely to be readmitted than low-risk.\".format(risk_ratio))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROJECT COMPLETE - ALL PHASES FINISHED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review Strategic Insight Report for business recommendations\")\n",
    "print(\"  2. Implement VCI in clinical workflow for discharge planning\")\n",
    "print(\"  3. Monitor VCI performance with prospective data\")\n",
    "print(\"  4. Refine risk thresholds based on operational capacity\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Export Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset with VCI scores\n",
    "output_file = 'VHN_Enhanced_Dataset_with_VCI.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"âœ“ Enhanced dataset saved to: {output_file}\")\n",
    "print(f\"  Includes VCI_Score and VCI_Risk_Category columns\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Total features: {len(df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
